--BEGIN--05/12/2025--DIVIDE AND CONQUER--
1. Divide and Conquer
ğ–¦¹ Break hard problems into smaller pieces
ğ–¦¹ Solve each small piece the same way (recursively)
ğ–¦¹ Combine answers from small pieces to get final answer
- Example: Sorting a huge pile by splitting in half, sorting halves, then merging (MergeSort)
ğ–¦¹ Faster than brute-force for big problems
- Brute-force like checking every pair of points takes nÂ² time
- Divide and conquer can reduce it to n.log n
ğ–¦¹ Analyze speed using recurrence relations
â€¢ Recurrence example: T(n) = 2T(n/2) + O(n)
- Means split job into 2 smaller jobs and do extra work to combine
- Solving gives time complexity like O(n.log n)
ğ–¦¹ Useful applications
â€¢ Finding two closest points on a map
â€¢ Multiplying very big numbers fast
â€¢ Comparing two ranked lists
â€¢ Cleaning noisy signals
ğ–¦¹ Big picture
- Dumb way is already polynomial (like nÂ²)
- Divide and conquer makes it much faster (n.log n or better)
- Goal: not just doable, but blazing fast

2. Karatsuba Multiplication
ğ–¦¹ Normal way for multiplying n-digit numbers needs nÂ² small multiplications
ğ–¦¹ Example: 123 Ã— 456 needs 9 small multiplications in the usual method
ğ–¦¹ Karatsuba splits each number into two halves for faster work
â€¢ Example split:
- 123 â†’ 12 | 3
- 456 â†’ 45 | 6
ğ–¦¹ Normal half-way method needs 4 multiplications:
â€¢ 12 Ã— 45
â€¢ 12 Ã— 6
â€¢ 3 Ã— 45
â€¢ 3 Ã— 6
ğ–¦¹ Karatsuba uses only 3 multiplications
â€¢ A = 12 Ã— 45
â€¢ B = 3 Ã— 6
â€¢ C = (12 + 3) Ã— (45 + 6)
- With A, B, C and simple add/subtract, we get the final answer (123 Ã— 456 = 56088)
ğ–¦¹ Fewer recursive steps give big speedup when numbers get very large
â€¢ Normal method keeps doing 4 half-size problems â†’ time is nÂ²
â€¢ Karatsuba keeps doing 3 half-size problems â†’ time is about nÂ¹Â·â¶
- This makes large-number multiplication much faster
â€¢ Formula using split pieces:
x = aÂ·B + b  
y = cÂ·B + d  
Compute:
[pink]p1[/pink] = [yellow]a[/yellow]Â·[yellow]c[/yellow]  
[pink]p2[/pink] = [yellow]b[/yellow]Â·[yellow]d[/yellow]  
[pink]p3[/pink] = ([yellow]a[/yellow] + [yellow]b[/yellow])([yellow]c[/yellow] + [yellow]d[/yellow])  
Middle term:
[pink]m[/pink] = [pink]p3[/pink] âˆ’ [pink]p1[/pink] âˆ’ [pink]p2[/pink]  
Combine:
xÂ·y = [pink]p1[/pink]Â·BÂ² + [pink]m[/pink]Â·B + [pink]p2[/pink]
- Time complexity: roughly nÂ¹Â·â¶  
- Space complexity: depends on recursion depth, about O(n)
--EMBED-GIT--
https://github.com/arihara-sudhan/dsa/blob/main/algorithms/karatsuba.py
Recurrence Relation: T(n)=3T(n/2)+O(n)
ğ–¦¹ Large example: 12345678 Ã— 87654321
â€¢ Level 0: two 8-digit numbers  
â€¢ Choose B = 10â´  
â€¢ Split:  
- 12345678 â†’ 1234 | 5678  
- 87654321 â†’ 8765 | 4321
ğ–¦¹ Karatsuba Level-1 (three 4-digit Ã— 4-digit problems)
â€¢ [pink]p1[/pink] = 1234 Ã— 8765  
â€¢ [pink]p2[/pink] = 5678 Ã— 4321  
â€¢ [pink]p3[/pink] = (1234 + 5678)(8765 + 4321) = 6912 Ã— 13086
ğ–¦¹ Karatsuba Level-2 (nine 2-digit Ã— 2-digit problems)
â€¢ Examples: 12Ã—87, 34Ã—65, 46Ã—152, 56Ã—43, 78Ã—21, 134Ã—64, 69Ã—130, 12Ã—86, 81Ã—216
ğ–¦¹ Karatsuba Level-3 (base case)
â€¢ Single-digit multiplications like 1Ã—8, 2Ã—7, 3Ã—5, etc.
- Total: 27 one-digit multiplications
- This is the stopping point
- Time complexity: about nÂ¹Â·â¶  
- Space complexity: depends on recursion depth, roughly O(n)

3. Strassen Matrix Multiplication
ğ–¦¹ Normal 2Ã—2 multiplication needs 8 multiplications
â€¢ Example: [ a b ] Ã— [ e f ] â†’ results:
  [ c d ]   [ g h ]  
- Each result: aÂ·e + bÂ·g, aÂ·f + bÂ·h, cÂ·e + dÂ·g, cÂ·f + dÂ·h â†’ total 8 multiplications
ğ–¦¹ For nÃ—n matrices â†’ nÂ³ multiplications
- Example: n = 1024 â†’ over a billion multiplications

ğ–¦¹ Strassen reduces 2Ã—2 multiplication to 7 multiplications
â€¢ M1 = (a + d)(e + h)  
â€¢ M2 = (c + d)Â·e  
â€¢ M3 = aÂ·(f âˆ’ h)  
â€¢ M4 = dÂ·(g âˆ’ e)  
â€¢ M5 = (a + b)Â·h  
â€¢ M6 = (c âˆ’ a)(e + f)  
â€¢ M7 = (b âˆ’ d)(g + h)
- Final 4 entries are built from M1â€¦M7 using only additions/subtractions
- 2Ã—2 multiplication: 8 â†’ 7 multiplications

ğ–¦¹ Divide-and-conquer for big matrices
â€¢ Split nÃ—n matrices into four (n/2)Ã—(n/2) blocks:  
  A = [ A11  A12 ]   B = [ B11  B12 ]  
      [ A21  A22 ]       [ B21  B22 ]
â€¢ Use 7-trick recursively on blocks
â€¢ Combine the seven results with additions/subtractions to get final matrix
--EMBED-GIT--
https://github.com/arihara-sudhan/dsa/blob/main/algorithms/strassen.py
ğ–¦¹ Recurrence relation
â€¢ T(n) = 7 Â· T(n/2) + O(nÂ²)  
- Only 7 recursive calls per level
- Extra O(nÂ²) = additions and subtractions of blocks

ğ–¦¹ Speed comparison
â€¢ Normal method: nÂ³ multiplications  
â€¢ Strassen method: roughly nÂ²Â·â¸â°â· multiplications  
- Example: 1024Ã—1024 â†’ ~10â¹ normal vs ~10â¸ Strassen â†’ 7â€“8Ã— faster  
- Example: 1,000,000Ã—1,000,000 â†’ hundreds of times faster
- Current best: nÂ²Â·Â³â·Â²â¸â¶, but Strassen started the revolution
- Time complexity: roughly nÂ²Â·â¸â°â·  
- Space complexity: O(nÂ²) for storing intermediate blocks
--END--
--BEGIN--05/12/2025--GREEDY TECHNIQUES--
1. Greedy Method
ğ–¦¹ Make a choice that looks best at the moment (local optimum)
ğ–¦¹ Repeat this step by step
ğ–¦¹ Hope these local choices lead to global best
- In short: "Take the best option now and keep going"
ğ–¦¹ Key idea
â€¢ Works step by step
â€¢ Does not go back or change previous choices
â€¢ Always chooses locally best option
- If problem has greedy-choice property â†’ gives global best
ğ–¦¹ Examples
â€¢ Coin Change Problem (with standard coins)
- Always pick largest coin â‰¤ remaining amount
â€¢ Fractional Knapsack Problem
- Pick items with highest value/weight ratio first
â€¢ Activity Selection Problem
- Pick next activity that finishes earliest
ğ–¦¹ Time Complexity
â€¢ Depends on how choices are selected
â€¢ O(n log n) if sorting is needed
â€¢ O(n) if no sorting is required

2. Huffman Coding
ğ–¦¹ Huffman coding compresses text by giving short codes to frequent characters and long codes to rare characters
ğ–¦¹ Goal: make the total message size as small as possible
ğ–¦¹ Steps to build Huffman code
â€¢ Step 1: Count frequency of each character
- Example "AABABC": A=3, B=2, C=1
â€¢ Step 2: Build tree using greedy rule
- Always pick two least frequent items and combine
- Example:
  - Start nodes: C(1), B(2), A(3)
  - Combine C(1) + B(2) â†’ Node(3)
  - Combine Node(3) + A(3) â†’ Root(6)
â€¢ Step 3: Assign binary codes from tree
- Go left â†’ 0, go right â†’ 1
- Example codes:
  - A = 0  
  - C = 10  
  - B = 11
â€¢ Step 4: Encode message
- "AABABC" â†’ 0 0 11 0 11 10 â†’ 001101110
- Original: 6 characters Ã— 8 bits = 48 bits  
- Huffman: 9 bits â†’ ~80% smaller
--EMBED-GIT--
https://github.com/arihara-sudhan/dsa/blob/main/algorithms/huffman.py
ğ–¦¹ Another example: "beep"
â€¢ Frequency: b=1, e=2, p=1
â€¢ Build tree:
  - Combine b(1)+p(1) â†’ Node(2)
  - Combine Node(2)+e(2) â†’ Root(4)
â€¢ Assign codes: e=1, b=00, p=01
â€¢ Encode "beep": b e e p â†’ 00 1 1 01 â†’ 001101
- Original 4Ã—8 = 32 bits â†’ Huffman 6 bits
- Time complexity: O(n log n) (for building tree with n characters)  
- Space complexity: O(n) (to store tree and codes)

3. Interval Scheduling (Maximum Non-Overlapping Intervals)
ğ–¦¹ Goal: Select the maximum number of intervals that do not overlap
ğ–¦¹ Correct greedy strategy:
â€¢ Sort intervals by ending time (earliest first)
â€¢ Pick the first interval
â€¢ Repeatedly pick next interval that starts after or when the last picked one ends
- Time complexity: O(n log n) due to sorting
- Optimal: YES
ğ–¦¹ Activity Selection Problem
â€¢ You are given n intervals [start_time, end_time]
â€¢ Select maximum number of non-overlapping intervals
â€¢ Golden greedy rule: always pick interval with earliest finish among remaining compatible intervals
ğ–¦¹ Step-by-step algorithm
â€¢ Sort all intervals by end time (ascending)
â€¢ Initialize: count = 0, last_end_time = -âˆ
â€¢ For each interval in sorted order:
  - If start_time â‰¥ last_end_time â†’ take it
  - Then count++, update last_end_time = end_time
â€¢ Return count
ğ–¦¹ Example 1
â€¢ Intervals: A(1-4), B(3-5), C(0-6), D(5-7), E(8-9), F(5-9), G(3-8)
â€¢ Sorted by end time: A(1-4), B(3-5), C(0-6), D(5-7), G(3-8), F(5-9), E(8-9)
â€¢ Greedy selection:
  - Pick A â†’ last_end = 4
  - Next compatible: D â†’ last_end = 7
  - Next compatible: E â†’ last_end = 9
â€¢ Selected: A, D, E â†’ 3 intervals (maximum possible)
--EMBED-GIT--
https://github.com/arihara-sudhan/dsa/blob/main/algorithms/activity_selection.py
ğ–¦¹ Example 2: Where other strategies fail
â€¢ Picking longest â†’ only 1 interval
â€¢ Picking shortest duration â†’ easy to mess order â†’ not always optimal
â€¢ Picking earliest start â†’ only 1 interval
â€¢ Correct greedy (earliest finish) â†’ picks A â†’ B â†’ C â†’ 3 intervals (optimal)
ğ–¦¹ Proof of optimality
â€¢ Theorem: earliest finish greedy always selects maximum intervals
â€¢ Proof idea: â€œGreedy stays aheadâ€ + exchange argument
â€¢ Let G = greedy solution, O = any optimal solution
â€¢ Sort both by finish time
â€¢ By induction, k-th interval in G finishes no later than k-th in O
- Therefore G fits at least as many intervals as O â†’ optimal

4. Set Cover Problem
ğ–¦¹ Goal: Find the fewest subsets whose union is the entire universe U
ğ–¦¹ Given:
â€¢ Universe U = {1,2,...,n}
â€¢ Family of subsets Sâ‚, Sâ‚‚,..., Sâ‚˜ âŠ† U
â€¢ Cost of each subset: usually 1 (unweighted) or cáµ¢ (weighted)
ğ–¦¹ Set Cover is NP-hard â†’ no exact polynomial-time solution
ğ–¦¹ Greedy algorithm gives best possible approximation
ğ–¦¹ Greedy Algorithm
--EMBED-GIT--
https://github.com/arihara-sudhan/dsa/blob/main/algorithms/set_cover.py
â€¢ Initialize: C = âˆ… (selected sets), Covered = âˆ…
â€¢ While Covered â‰  U:
  - Pick set Sâ±¼ that covers the most new elements not yet covered
  - Add Sâ±¼ to C
  - Update Covered = Covered âˆª Sâ±¼
  - Remove elements in Sâ±¼ from consideration
â€¢ Return C
ğ–¦¹ Two versions of Set Cover
â€¢ Unweighted: minimize number of sets, all sets cost same
  - Greedy: pick set covering most new elements
â€¢ Weighted: minimize total cost, each set has a price
  - Greedy: pick set with best ratio (new elements covered Ã· cost)
ğ–¦¹ Example 1: Unweighted Set Cover
â€¢ Universe = {1,2,3,4,5,6}, Sets:
  - A={1,2,3,4}, B={3,4,5,6}, C={1,2}, D={3,4}, E={5,6}
â€¢ Step-by-step greedy:
  - Pick A â†’ covers {1,2,3,4}
  - Pick B â†’ covers {5,6}
â€¢ Total sets used = 2 (optimal)
ğ–¦¹ Example 2: Weighted Set Cover
â€¢ Same universe, sets with costs:
  - A=100, B=100, C=10, D=10, E=10
â€¢ Step-by-step weighted greedy (pick max new/cost):
  - Ratios: C=0.2, D=0.2, E=0.2, A=0.04
  - Pick C, D, E â†’ total cost = 30 (optimal)
â€¢ Wrong unweighted greedy (pick most new elements) â†’ total cost 110
ğ–¦¹ Approximation Guarantee
â€¢ Greedy solution uses at most Hâ‚™ Ã— OPT sets
  - OPT = size of optimal cover
  - Hâ‚™ = 1 + 1/2 + 1/3 + ... + 1/n â‰ˆ ln n + 1
â€¢ Best possible: no algorithm can do better than ln n-approximation unless P=NP
â€¢ For n â‰¤ 1000 â†’ Hâ‚™ â‰¤ ~7 â†’ greedy at most 7Ã— optimal
ğ–¦¹ Time Complexity: O(mn) for unweighted, can be improved with heaps  
ğ–¦¹ Space Complexity: O(n + m) (to track covered elements and sets)
--END--
--BEGIN--05/12/2025--DYNAMIC PROGRAMMING--
1. What is Dynamic Programming
ğ–¦¹ Dynamic Programming (DP) solves problems by breaking them into smaller subproblems and storing their results to avoid repeating work
ğ–¦¹ Used when a problem has overlapping subproblems and optimal substructure
ğ–¦¹ Key Features
â€¢ Optimal Substructure
- Solution of problem can be built from solutions of subproblems
- Example: shortest path in a graph uses shortest paths of smaller segments
â€¢ Overlapping Subproblems
- Same subproblems appear multiple times
- Example: Fibonacci numbers â†’ fib(n-1) and fib(n-2) are reused

2. Difference from Divide and Conquer
ğ–¦¹ Subproblems: Divide & Conquer = independent, DP = overlapping
ğ–¦¹ Strategy: Divide & Conquer = solve recursively, DP = solve recursively or iteratively + store results
ğ–¦¹ Time Complexity: Divide & Conquer often O(2^n), DP usually polynomial O(n), O(nÂ²), etc.
ğ–¦¹ Example: Divide & Conquer = Merge Sort, DP = Fibonacci, Knapsack

3. Two Approaches in DP
A. Top-Down (Memoization)
ğ–¦¹ Solve main problem recursively
ğ–¦¹ Store results of subproblems in cache (dictionary or array)
ğ–¦¹ Before solving subproblem, check if result exists
ğ–¦¹ Example: Fibonacci Numbers
[pink]def[/pink] [yellow]fib_memo(n, memo={}):[/yellow]
    [pink]if[/pink] [yellow]n in memo:[/yellow]
        [pink]return[/pink] [yellow]memo[n][/yellow]
    [pink]if[/pink] [yellow]n <= 1:[/yellow]
        [pink]return[/pink] [yellow]n[/yellow]
    [yellow]memo[n] = fib_memo(n-1, memo) + fib_memo(n-2, memo)[/yellow]
    [pink]return[/pink] [yellow]memo[n][/yellow]
[ pink]print[/pink]([yellow]fib_memo(10)[/yellow])  # 55
ğ–¦¹ Time Complexity: O(n)
ğ–¦¹ Space Complexity: O(n) (for memo + recursion stack)

B. Bottom-Up (Tabulation)
ğ–¦¹ Solve smallest subproblems first
ğ–¦¹ Build solution iteratively
ğ–¦¹ Often uses arrays to store intermediate results
ğ–¦¹ Example: Fibonacci Numbers
[pink]def[/pink] [yellow]fib_tab(n):[/yellow]
    [pink]if[/pink] [yellow]n <= 1:[/yellow]
        [pink]return[/pink] [yellow]n[/yellow]
    [yellow]dp = [0]*(n+1)[/yellow]
    [yellow]dp[0], dp[1] = 0, 1[/yellow]
    [pink]for[/pink] [yellow]i in range(2, n+1):[/yellow]
        [yellow]dp[i] = dp[i-1] + dp[i-2][/yellow]
    [pink]return[/pink] [yellow]dp[n][/yellow]
[pink]print[/pink]([yellow]fib_tab(10)[/yellow])  # 55
ğ–¦¹ Time Complexity: O(n)
ğ–¦¹ Space Complexity: O(n) (can reduce to O(1) using two variables)

4. Common DP Problems
ğ–¦¹ Fibonacci sequence
â€¢ Subproblem: fib(n) = fib(n-1) + fib(n-2)
ğ–¦¹ 0/1 Knapsack Problem
â€¢ Subproblem: max value for first i items and capacity w
ğ–¦¹ Longest Common Subsequence (LCS)
â€¢ Subproblem: LCS of first i characters of string1 and first j of string2
ğ–¦¹ Matrix Chain Multiplication
â€¢ Subproblem: minimum multiplications for matrices i to j
ğ–¦¹ Coin Change Problem
â€¢ Subproblem: min number of coins to make amount x

5. Matrix Chain Multiplication (MCM)
ğ–¦¹ Problem Statement
â€¢ Given matrices A1, A2, ..., An with dimensions pi-1 Ã— pi
â€¢ Goal: Multiply all matrices A1 Ã— A2 Ã— ... Ã— An with minimum scalar multiplications
- Only multiplication cost matters, not the resulting matrix
ğ–¦¹ Example
â€¢ A1: 10Ã—30, A2: 30Ã—5, A3: 5Ã—60
â€¢ (A1 Ã— A2) Ã— A3 â†’ cost = 10Ã—30Ã—5 + 10Ã—5Ã—60 = 4500 (optimal)
â€¢ A1 Ã— (A2 Ã— A3) â†’ cost = 30Ã—5Ã—60 + 10Ã—30Ã—60 = 27000
--EMBED-GIT--
https://github.com/arihara-sudhan/dsa/blob/main/algorithms/matrix_chain_multiplication.py
ğ–¦¹ Key Idea
â€¢ Optimal Substructure
- Minimum cost for multiplying matrices i to j = min over all k (i â‰¤ k < j) of:
  m[i][k] + m[k+1][j] + pi-1 Ã— pk Ã— pj
- Base case: m[i][i] = 0 (single matrix, no multiplication)
â€¢ Overlapping Subproblems
- Subchains like A1Ã—A2, A2Ã—A3 are used multiple times
ğ–¦¹ DP Table
â€¢ Use dp[i][j] to store minimum cost for multiplying matrices i to j
â€¢ Fill table by increasing chain length from 2 to n
ğ–¦¹ Recurrence Relation
dp[i][j] = min over k = i to j-1 of (dp[i][k] + dp[k+1][j] + pi-1 Ã— pk Ã— pj)
â€¢ Iterate i = nâ€¦1
â€¢ Iterate j = i+1â€¦n
â€¢ Iterate k = iâ€¦j-1
ğ–¦¹ Complexity
â€¢ Time Complexity: O(nÂ³) â†’ 3 nested loops
â€¢ Space Complexity: O(nÂ²) â†’ DP table

6. Floyd-Warshall Algorithm
ğ–¦¹ Problem Statement
â€¢ Given a weighted directed graph with n vertices
â€¢ Some edges may be missing (no direct connection)
â€¢ Goal: Find shortest distances between all pairs of vertices
- Works with negative edge weights if no negative cycles exist
- Finds all-pairs shortest paths unlike Dijkstra
ğ–¦¹ Key Idea (Dynamic Programming)
â€¢ Let dist[i][j] = shortest distance from vertex i to vertex j
â€¢ Consider vertices 1 to k as possible intermediate points
â€¢ Recurrence:
  dist[i][j][k] = min(dist[i][j][k-1], dist[i][k][k-1] + dist[k][j][k-1])
- dist[i][j][k-1] â†’ shortest distance without using vertex k
- dist[i][k][k-1] + dist[k][j][k-1] â†’ shortest distance through vertex k
â€¢ Simpler version:
- Start with dist[i][j] = weight[i][j] (âˆ if no edge)
- For each intermediate vertex k = 0..n-1:
  â€¢ For each pair (i, j):
    dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])
- After all vertices considered, dist[i][j] = shortest path from i to j
--EMBED-GIT--
https://github.com/arihara-sudhan/dsa/blob/main/algorithms/floyd_warshall.py
ğ–¦¹ Example
â€¢ Graph with 4 vertices (0 to 3)
Edges:
0 â†’ 1 : 5, 0 â†’ 3 : 10, 1 â†’ 2 : 3, 2 â†’ 3 : 1
â€¢ Initial distance matrix:
From\To 0  1  2  3
0       0  5  âˆ 10
1       âˆ  0  3  âˆ
2       âˆ  âˆ 0  1
3       âˆ  âˆ  âˆ 0
â€¢ Run algorithm â†’ update distances using each vertex as intermediate
â€¢ Final distance matrix:
From\To 0  1  2  3
0       0  5  8  9
1       âˆ  0  3  4
2       âˆ  âˆ 0  1
3       âˆ  âˆ  âˆ 0
ğ–¦¹ Complexity
â€¢ Time Complexity: O(nÂ³) â†’ 3 nested loops
â€¢ Space Complexity: O(nÂ²) â†’ distance matrix
--END--
--BEGIN--12/13/2025--RANDOMIZED ALGORITHMS--
1. Randomized Algorithms
ğ–¦¹ Randomized algorithms use randomness to improve performance, especially in average-case scenarios or against difficult inputs.
ğ–¦¹ They use random bits or pseudorandom generators, making runtime or output probabilistic.
ğ–¦¹ Types:
â€¢ Las Vegas: always correct, runtime may vary.
â€¢ Monte Carlo: fixed runtime, small probability of error, often used for approximation.
ğ–¦¹ Monte Carlo error is bounded by Îµ with probability 1âˆ’Î´; increasing sample size reduces error as 1/âˆšN.
ğ–¦¹ Monte Carlo method steps:
â€¢ Define input domain.
â€¢ Generate random samples.
â€¢ Compute deterministically.
â€¢ Aggregate results.
- Convergence is guaranteed by the law of large numbers.
ğ–¦¹ Variants: Markov Chain Monte Carlo (sampling distributions), Importance Sampling (biased regions).
ğ–¦¹ Examples:
â€¢ Approximate Ï€ by scattering points in a unit square and counting inside a quarter circle.
â€¢ Kargerâ€™s Min-Cut algorithm contracts random edges to find minimum cuts.
â€¢ Randomized Quicksort chooses a random pivot for expected O(n log n) time.
â€¢ Simulate dice throws by averaging over many trials.
ğ–¦¹ Monte Carlo expected value algorithm: run N samples and average results.
â€¢ Time complexity: O(N Ã— computation per sample)
â€¢ Space complexity: low
ğ–¦¹ Derandomization: convert to deterministic via conditional probabilities or expanders, but may be inefficient.
ğ–¦¹ Complexity classes:
â€¢ BPP: bounded error, polynomial time
â€¢ RP: one-sided error
ğ–¦¹ Applications: optimization, physics simulations, integration, AI (Monte Carlo Tree Search), cryptography.

2. Pseudorandom Number Generation
ğ–¦¹ Computers generate pseudorandom numbers using formulas like Linear Congruential Generator (LCG): [pink]xi+1 = (A â‹… xi) mod M[/pink]
â€¢ xi = current number, xi+1 = next number, A = multiplier, M = modulus.
â€¢ Start from seed x0; each next number = A * current number mod M.
ğ–¦¹ Example: M = 11, A = 7, x0 = 1 â†’ sequence: 1, 7, 5, 2, 3, ...
ğ–¦¹ Proper choice of A and M ensures long period; sequence is reproducible if seed known.
ğ–¦¹ Modern libraries provide generators like LCG, Mersenne Twister.
â€¢ Distributions like uniform or normal can be applied.

3. Skip Lists
ğ–¦¹ Skip lists are randomized data structures for fast search and insertion.
ğ–¦¹ Linked lists alone: O(N) search.
ğ–¦¹ Skip lists add shortcut links to skip nodes.
ğ–¦¹ Node levels are chosen randomly:
â€¢ Flip a coin until heads appear; number of flips = node level.
â€¢ About 50% level 1, 25% level 2, 12.5% level 3, etc.
ğ–¦¹ Search starts at highest level and drops down as needed.
â€¢ Expected search and insertion time: O(log N)
â€¢ Space complexity: O(N)
ğ–¦¹ Skip lists are efficient and robust against worst-case inputs.

4. Randomized Primality Testing
ğ–¦¹ Checks if large number N is prime faster than naive methods.
ğ–¦¹ Fermatâ€™s Little Theorem: for prime P and 0 < A < P, [pink]A^(Pâˆ’1) â‰¡ 1 mod P[/pink]
ğ–¦¹ Algorithm:
â€¢ Pick A randomly between 1 and Nâˆ’1.
â€¢ If [pink]A^(Nâˆ’1) â‰¡ 1 mod N[/pink] fails â†’ N is composite.
â€¢ If holds â†’ N is probably prime.
ğ–¦¹ Randomization reduces being fooled by Carmichael numbers.
â€¢ Carmichael numbers: composite but satisfy [pink]A^(Nâˆ’1) â‰¡ 1 mod N[/pink] for all A coprime to N.
â€¢ Examples: 561, 1105, 1729, 2465.
ğ–¦¹ Multiple random trials reduce error probability to very small.
ğ–¦¹ Randomized tests like Miller-Rabin ensure fast and reliable primality checking.
â€¢ Time complexity: polynomial in number of digits
â€¢ Space complexity: low
--END--
--BEGIN--14/12/2025--PARAL & DIST ALGORITHMS--
1. PRAM
ğ–¦¹ PRAM is a simple model to design parallel algorithms
ğ–¦¹ Many processors work at the same time in sync
ğ–¦¹ All processors can access the same shared memory instantly
ğ–¦¹ It assumes no delay, no communication cost, and no memory conflict
â€¢ Cost measures
- Parallel time means number of global steps
- Total work means parallel_time Ã— number_of_processors
â€¢ Memory access types
- EREW: no two processors can read or write the same memory at the same time
- CREW: many can read the same memory, but only one can write
- CRCW: many can read and write the same memory
â€¢ Write conflict rules in CRCW
- Common: all write the same value
- Arbitrary: any one writer wins
- Priority: highest priority wins
- Reduction: values are combined like sum or max
ğ–¦¹ Used mainly to think about parallel limits, not real hardware
ğ–¦¹ Inspires multicore CPUs and GPUs
â€¢ Complexity and limits
- Time: often O(log n) for problems like sorting
- Space: assumes unlimited shared memory
- Limits: ignores cache, delay, and real hardware issues
â€¢ Comparison
- PRAM uses shared memory
- Distributed systems use message passing

2. DIVIDE AND CONQUER
ğ–¦¹ Break a problem into smaller independent parts
ğ–¦¹ Solve each part
ğ–¦¹ Combine all answers
â€¢ Basic idea
- Divide data into parts
- Solve parts using recursion
- Combine results
â€¢ Parallel version
- Independent subproblems run at the same time
- Left and right parts are solved together
- Works well with PRAM and distributed systems
â€¢ Parallel steps
- Divide: split data into chunks (often O(1) or O(n/p))
- Conquer: each chunk runs on different processors
- Combine: merge partial results
â€¢ Recursion tree view
- Each level has more subproblems
- All problems in one level run in parallel
- Number of levels is log n
- Parallel time becomes O(log n) if enough processors exist
â€¢ Models
- Shared memory: easy access and merging
- Distributed: local memory and message passing
â€¢ Techniques
- Team parallel model: processors split into groups for each subproblem
- Work stealing: idle processors take tasks from busy ones
- Communication minimization: reduce data transfer during combine
â€¢ Examples
- Parallel quicksort
  â€¢ Independent left and right parts
  â€¢ Parallel time: O(log n)
  â€¢ Total work: O(n log n)
- Parallel mergesort
  â€¢ Sort halves in parallel
  â€¢ Merge can also be parallel
  â€¢ Parallel time: O(logÂ² n)
  â€¢ Work: O(n log n)
- Matrix multiplication
  â€¢ Split matrices into submatrices
  â€¢ Independent multiplications run in parallel
â€¢ Key terms
- Parallel time: longest dependency chain
- Work: total operations
- Efficiency: work / time
â€¢ Why powerful
- Naturally parallel
- Scales well
- Used in sorting, FFT, and matrix algorithms

3. LOAD BALANCING
ğ–¦¹ Load balancing means spreading work evenly across processors
ğ–¦¹ Goal is to avoid idle processors and reduce finish time
ğ–¦¹ Used in parallel and distributed systems
â€¢ Types
- Static: tasks assigned before execution
  â€¢ Low overhead
  â€¢ Poor for uneven tasks
- Dynamic: tasks assigned during execution
  â€¢ Better balance
  â€¢ Has communication cost
â€¢ Common methods
- Round robin
  â€¢ Tasks given one by one in order
  â€¢ Simple but may cause imbalance
- Power of two choices
  â€¢ Pick two processors randomly
  â€¢ Assign task to less loaded one
  â€¢ Very good balance with low cost
- Work stealing
  â€¢ Each processor has its own task list
  â€¢ Idle ones steal tasks
  â€¢ Scalable and decentralized
- Masterâ€“worker
  â€¢ One master assigns tasks
  â€¢ Can become a bottleneck
â€¢ Prefix sum based method
- Used when task sizes are known
- Divides work evenly
- Parallel time: O(log n)
â€¢ Applications
- Clusters and cloud systems
- Parallel sorting
- Multicore runtimes
â€¢ Complexity reminder
- Time depends on balance quality
- Space includes task queues and buffers
--END--
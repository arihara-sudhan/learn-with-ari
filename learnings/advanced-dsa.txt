--BEGIN--22/11/2025--TIME COMPLEXITY--
1. Computational and asymptotic complexity
ğ–¦¹ A single problem can have many algorithms, each with different efficiency.
ğ–¦¹ For small data, these differences are small; for large data, the differences grow.
â€¢ Computational complexity
- Shows how much effort an algorithm needs in terms of time or space.

2. Time and space as efficiency measures
ğ–¦¹ Time is usually more important than space when judging an algorithm.
ğ–¦¹ Actual run time depends on the machine used.
â€¢ System dependence
- An inefficient algorithm on a very fast machine may still run faster than an efficient one on a slow machine.
- To compare many algorithms, all must be tested on the same machine.
â€¢ Language dependence
- Compiled programs run faster than interpreted ones.
- Some languages produce faster code than others.

3. Using logical units instead of real time
ğ–¦¹ Real-time units (microseconds, nanoseconds) should not be used for analysis.
ğ–¦¹ Use logical units that relate input size n to time t.
â€¢ Linear example
- If tâ‚ = cÂ·nâ‚, then increasing n by 5 times increases t by 5 times.
â€¢ Log example
- If tâ‚ = logâ‚‚(n), doubling n increases t by 1 unit.

4. Need for simplified functions
ğ–¦¹ The true relationship between n and t can be complex.
ğ–¦¹ For large data, small terms in the function do not change its overall growth.
ğ–¦¹ These small terms are removed to get a simpler approximate function.
â€¢ Asymptotic complexity
- Used when only the main growth of the function matters.
- Useful when exact calculation is hard or unnecessary.

5. Example: f(n) = nÂ² + 100n + logâ‚â‚€(n) + 1000
ğ–¦¹ For small n, the constant term 1000 is the biggest.
ğ–¦¹ At n = 10, the linear term 100n and constant term 1000 are similar in effect.
ğ–¦¹ At n = 100, the nÂ² and 100n terms have similar effects.
â€¢ For large n
- The nÂ² term grows much faster than other terms.
- 100n, logâ‚â‚€(n), and 1000 become very small in comparison.
- The function mainly depends on nÂ² when n is large.

6. Big-O notation
ğ–¦¹ Big-O is used to describe how fast a function grows for large n.
ğ–¦¹ It compares two functions f(n) and g(n) by checking if f grows no faster than g.
â€¢ Meaning of f(n) = O(g(n))
IMG_URL fnlessthancgn.png
- There exist positive numbers c and N such that f(n) â‰¤ cÂ·g(n) for all n â‰¥ N.
IMG_URL fnogn.png
- g(n) acts as an upper bound for the growth of f(n) when n is large.
IMG_URL rabbit-carrot-chase.gif

7. Limits of the basic definition
ğ–¦¹ The definition says constants c and N must exist but does not show how to find them.
ğ–¦¹ Many different pairs of c and N can satisfy the same f(n) and g(n).
â€¢ Example idea
- For f(n) = 2nÂ² + 3n + 1 and g(n) = nÂ², many c, N pairs work.
- These pairs come from solving: 2nÂ² + 3n + 1 â‰¤ cÂ·nÂ².

8. Finding useful constants
ğ–¦¹ To pick the best N, find when the largest term in f(n) stays the largest.
ğ–¦¹ For f(n) = 2nÂ² + 3n + 1, the biggest terms are 2nÂ² and 3n.
â€¢ Compare terms
- 2nÂ² > 3n when n > 1.
- So N = 2 is suitable, and c must be at least 33/4.

9. Meaning of different c and N
ğ–¦¹ Many pairs of c and N still describe the same relationship f = O(g).
ğ–¦¹ â€œAlmost alwaysâ€ means gÂ·c is â‰¥ f for all n â‰¥ N.
â€¢ Effect of choosing different N
- If N = 1, then c must be â‰¥ 6.
- If N = 2, c must be â‰¥ 3.75.
- If N = 3, c must be â‰¥ 31/9.
- c and N depend on each other.

10. Multiple possible upper bounds
ğ–¦¹ A function f can be O(nÂ²), O(nÂ³), O(nâ´), â€¦ for any power â‰¥ 2.
ğ–¦¹ To avoid too many choices, the smallest suitable g(n) is normally taken.
â€¢ Removing small terms
- Small terms in f(n) that do not affect growth are written using O( ).
- Example: f(n) = nÂ² + 100n + O(logâ‚â‚€(n))
â€¢ Another example
- f(n) = 2nÂ² + O(n)

11. Finding c and N
IMG_URL bigo.png
ğ–¦¹ We compare each part of f(n) with g(n) to show f(n) â‰¤ c Ã— g(n)
ğ–¦¹ We rewrite all parts of f(n) so they look like some_number Ã— g(n)
â€¢ This helps us show f(n) grows no faster than g(n)
- Explanation: If every part of f(n) can be written in terms of g(n), then the whole f(n) can also be written using g(n)

11.1 Making all terms look like g(n) = nÂ²
ğ–¦¹ For f(n) = 2nÂ² + 3n + 1, we try to rewrite each term as something Ã— nÂ²
ğ–¦¹ First term: 2nÂ² â‰¤ 2nÂ²
â€¢ It already matches nÂ², so no change needed

11.2 Converting 3n into something Ã— nÂ²
ğ–¦¹ We check if 3n can be compared with 3nÂ²
ğ–¦¹ Check if n â‰¤ nÂ² when n â‰¥ 1
â€¢ Divide both sides by n (n > 0) â†’ 1 â‰¤ n
- Explanation: This is true for all n â‰¥ 1, so 3n â‰¤ 3nÂ²

11.3 Converting constant 1 into something Ã— nÂ²
ğ–¦¹ We check if 1 â‰¤ nÂ² when n â‰¥ 1
ğ–¦¹ For n = 1,2,3,... this is always true
â€¢ So 1 â‰¤ nÂ² for all n â‰¥ 1

11.4 Putting all replacements together
ğ–¦¹ Now each part of f(n) is written in terms of nÂ²
ğ–¦¹ So:
â€¢ 2nÂ² + 3n + 1 â‰¤ 2nÂ² + 3nÂ² + nÂ²
- Explanation: Add them â†’ 6nÂ²

11.5 Final values
ğ–¦¹ f(n) â‰¤ 6nÂ²
ğ–¦¹ So c = 6
ğ–¦¹ And N = 1

12. Properties of Big-O notation
ğ–¦¹ Big-O has useful rules that help compare how fast functions grow
ğ–¦¹ These rules make it easier to estimate how fast algorithms run

12.1 Fact 1: Transitivity
IMG_URL transitive.png
ğ–¦¹ If f(n) is O(g(n)) and g(n) is O(h(n)), then f(n) is O(h(n))
ğ–¦¹ This means we can connect the two bounds into one
- Explanation: Since f(n) â‰¤ c1Â·g(n) for n â‰¥ N1 and g(n) â‰¤ c2Â·h(n) for n â‰¥ N2, we get f(n) â‰¤ (c1Â·c2)Â·h(n) for large enough n

12.2 Fact 2: Sum rule
IMG_URL sumrule.png
ğ–¦¹ If f(n) is O(h(n)) and g(n) is O(h(n)), then f(n) + g(n) is O(h(n))
- Explanation: If f(n) â‰¤ c1Â·h(n) and g(n) â‰¤ c2Â·h(n), then f(n)+g(n) â‰¤ (c1+c2)Â·h(n)

12.3 Fact 3: Constant times nk
IMG_URL constant.png
ğ–¦¹ The function aÂ·n^k is O(n^k)
- Explanation: aÂ·n^k â‰¤ cÂ·n^k if c â‰¥ a
An Example: Even though 5Â·nÂ³ is big for small n, as n â†’ âˆ, it becomes insignificant compared to nâ´. Multiplying by a constant (5) cannot change the exponent â€” nÂ³ will always grow slower than nâ´ in the long run

12.4 Fact 4: Smaller powers grow slower
IMG_URL smallpower.png
ğ–¦¹ n^k is O(n^(k+j)) for any positive j
- Explanation: This works for c = 1 and N = 1

12.5 Polynomial rule
IMG_URL highestpower.png
ğ–¦¹ A polynomial grows at the rate of its highest power
ğ–¦¹ Example: f(n) = akÂ·n^k + akâˆ’1Â·n^(kâˆ’1) + â€¦ + a1Â·n + a0 is O(n^k)
â€¢ Also, f(n) is O(n^(k+j)) for any positive j

12.6 Logarithmic functions grow very slowly
ğ–¦¹ Algorithms with logarithmic running time are considered very good
ğ–¦¹ Some even slower functions exist, like log log n or constant time O(1), but they are rarely used

12.7 Fact 5: Constant multiple rule
ğ–¦¹ If f(n) = cÂ·g(n), then f(n) is O(g(n))
- Explanation: Multiplying by a fixed constant does not change the growth rate

12.8 Fact 6: Logarithms with any base have the same growth
IMG_URL logdiffbases.png
ğ–¦¹ log_a(n) is O(log_b(n)) for any positive a and b â‰  1
ğ–¦¹ All logarithms grow at the same rate
- Explanation: Using log rules, log_a(n) can be written as a constant Ã— log_b(n)

12.9 Fact 7: Using base 2 for convenience
ğ–¦¹ log_a(n) is O(lg n) for any positive a â‰  1
ğ–¦¹ lg n means log base 2 of n

13. Î© and Î˜ notations
ğ–¦¹ Î© notation gives a lower bound for how fast a function grows
ğ–¦¹ f(n) is Î©(g(n)) when f(n) â‰¥ cÂ·g(n) for all n â‰¥ N
â€¢ Means f grows at least as fast as g in the long run
- Explanation: Only the direction of the inequality changes compared to big-O

13.1 Relation between Î© and O
ğ–¦¹ f(n) is Î©(g(n)) exactly when g(n) is O(f(n))
ğ–¦¹ Both notations allow many choices of c and N
â€¢ Many different lower bounds are possible for the same f

13.2 Examples of many possible lower bounds
ğ–¦¹ If f(n) is Î©(nÂ²), it is also Î©(n), Î©(nÂ¹áŸÂ²), Î©(nÂ¹áŸÂ³), and so on
ğ–¦¹ Also Î©(log n), Î©(log log n), etc.
- Explanation: Only the largest lower bound is interesting in practice

13.3 Î˜ notation idea
ğ–¦¹ Î˜ notation gives both lower and upper bounds together
ğ–¦¹ f(n) is Î˜(g(n)) if c1Â·g(n) â‰¤ f(n) â‰¤ c2Â·g(n) for all n â‰¥ N
- Explanation: Means f and g grow at the same rate in the long run

13.4 Link between O, Î©, and Î˜
ğ–¦¹ f(n) is Î˜(g(n)) only if f(n) is both O(g(n)) and Î©(g(n))
ğ–¦¹ For the function 2nÂ² + 3n + 1, the simplest Î˜ bound is nÂ²

14. Possible problems with big-O
ğ–¦¹ Big-O hides constants; a large constant can mislead comparisons
ğ–¦¹ Some algorithms may look worse by big-O but are faster for small n
- Explanation: Big-O only cares about the long run and ignores small n

14.1 Example of misleading big-O
ğ–¦¹ Compare 108Â·n and 10Â·nÂ²
ğ–¦¹ First is O(n), second is O(nÂ²)
â€¢ For n â‰¤ 10â·, second one is actually faster
- Explanation: Big-O alone may cause rejecting a better algorithm

14.2 Double-O (OO) notation idea
ğ–¦¹ OO(g(n)) means O(g(n)) but with a constant too large to be useful
ğ–¦¹ Example: 108Â·n is OO(n)
- Explanation: â€œToo largeâ€ depends on the application

15. Examples of complexities
ğ–¦¹ Algorithms can be grouped by how fast their running time grows
ğ–¦¹ Common groups: constant, logarithmic, linear, nÂ·log n, quadratic, cubic, exponential
- Explanation: Growth patterns show which algorithms are practical

15.1 Real-time execution examples
ğ–¦¹ With 1 million operations per second, quadratic on 1M items takes 11+ days
ğ–¦¹ Cubic on 1M items takes thousands of years
â€¢ Even huge hardware improvements help only a little for high-complexity algorithms
- Explanation: Good algorithm design is more important than fast hardware

15.2 Importance of complexity study
ğ–¦¹ Even fast computers cannot save badly designed algorithms
ğ–¦¹ Understanding complexity is essential, especially in data structures

16. Asymptotic complexity idea
ğ–¦¹ Asymptotic bounds help us estimate time and memory needs of algorithms
ğ–¦¹ We mostly study time complexity by counting assignment steps
â€¢ Comparisons are also counted in some cases
- Explanation: We focus on how the total work grows when input size grows

16.1 Simple loop example
ğ–¦¹ Code:
[pink]for[/pink] [yellow](i = sum = 0; i < n; i++)[/yellow]
    [pink]sum[/pink] [yellow]+= a[i];[/yellow]
ğ–¦¹ Two initial assignments, then loop runs n times
â€¢ Each loop step has 2 assignments (sum update and i update)
- Explanation: Total steps = 2 + 2n, so complexity is O(n)

16.2 Nested loop example (sums of subarrays starting at 0)
ğ–¦¹ Code:
[pink]for[/pink] [yellow](i = 0; i < n; i++)[/yellow] {
    [pink]for[/pink] [yellow](j = 1, sum = a[0]; j <= i; j++)[/yellow]
        [pink]sum[/pink] [yellow]+= a[j];[/yellow]
    [yellow]System.out.println(...);[/yellow]
}
ğ–¦¹ Outer loop runs n times; inner loop runs i times for each i
â€¢ Inner loop has 2 assignments per step
- Explanation: Total = 1 + 3n + 2(1+2+â€¦+(nâ€“1)) = O(nÂ²)

16.3 Nested loops but with fixed-size work
ğ–¦¹ We print sums of last five cells of each subarray starting at 0
ğ–¦¹ Code:
[pink]for[/pink] [yellow](i = 4; i < n; i++)[/yellow] {
    [pink]for[/pink] [yellow](j = i-3, sum = a[i-4]; j <= i; j++)[/yellow]
        [pink]sum[/pink] [yellow]+= a[j];[/yellow]
    [yellow]System.out.println(...);[/yellow]
}
â€¢ Outer loop runs nâ€“4 times
â€¢ Inner loop always runs 4 times (constant work)
- Explanation: Total = 1 + 8Â·(nâ€“4) = O(n)

16.4 Loops where iteration count depends on data order
ğ–¦¹ We find length of longest increasing subarray
ğ–¦¹ Code:
[pink]for[/pink] [yellow](i = 0, length = 1; i < n-1; i++)[/yellow] {
    [pink]for[/pink] [yellow](i1 = i2 = k = i; k < n-1 && a[k] < a[k+1]; k++, i2++)[/yellow];
    [pink]if[/pink] [yellow](length < i2 - i1 + 1)[/yellow]
        [pink]length[/pink] [yellow]= i2 - i1 + 1;[/yellow]
    [yellow]System.out.println(...);[/yellow]
}
ğ–¦¹ Worst case (array strictly increasing):
â€¢ Outer loop runs nâ€“1 times, inner loop runs (nâ€“1â€“i) times  
- Explanation: Total = O(nÂ²)
ğ–¦¹ Best case (array strictly decreasing):
â€¢ Inner loop runs once each time  
- Explanation: Total = O(n)

16.5 Binary search complexity
ğ–¦¹ Binary search works by checking the middle of the current array part
ğ–¦¹ Code:
[pink]int[/pink] [yellow]binarySearch(int[] arr, int key) {[/yellow]
    [pink]int[/pink] [yellow]lo = 0, mid, hi = arr.length-1;[/yellow]
    [pink]while[/pink] [yellow](lo <= hi) {[/yellow]
        [yellow]mid = (lo + hi)/2;[/yellow]
        [pink]if[/pink] [yellow](key < arr[mid])[/yellow]
            [yellow]hi = mid - 1;[/yellow]
        [pink]else if[/pink] [yellow](arr[mid] < key)[/yellow]
            [yellow]lo = mid + 1;[/yellow]
        [pink]else return[/pink] [yellow]mid;[/yellow]
    [yellow]}[/yellow]
    [pink]return[/pink] [yellow]-1;[/yellow]
[yellow]}[/yellow]
ğ–¦¹ If key is absent:
â€¢ Checked array sizes go n â†’ n/2 â†’ n/4 â†’ ... â†’ 1
- Explanation: Number of halvings = m where n / 2^m = 1, so m = logâ‚‚ n  
â€¢ Complexity = O(log n)
--END--

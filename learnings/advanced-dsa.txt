IMG_URL sparse_array.png
1. Sparse Arrays
ð–¦¹ Concept: An array where most elements are zero or empty (null, None, etc.).
ð–¦¹ Example: 
â€¢ Regular array (size 10): [0, 0, 0, 5, 0, 0, 0, 0, 0, 10]
â€¢ Sparse array: Only non-zero values stored: index 3 â†’ 5, index 9 â†’ 10
ð–¦¹ Reason: Storing lots of zeros wastes memory, especially in large arrays.

2. Problem with Regular Arrays
ð–¦¹ Memory wasted storing zeros.
ð–¦¹ Traversal or copying wastes time on zeros.
ð–¦¹ Large arrays may exceed memory capacity if most entries are empty.

3. Solution: Store Only Non-empty Elements
ð–¦¹ Store only non-zero or meaningful elements along with their indices.
ð–¦¹ Reduces memory usage drastically.

4. Implementations
ð–¦¹ a) Hash Map / Dictionary
â€¢ Idea: Use key-value store: Key = index, Value = non-zero element.
â€¢ Example in Python:
<pink># Original sparse array</pink>
<yellow>arr = [0, 0, 0, 5, 0, 0, 0, 0, 0, 10]</yellow>
<pink># Sparse representation using dict</pink>
<yellow>sparse_dict = {3: 5, 9: 10}</yellow>
<pink># Access element at index i</pink>
<yellow>i = 3</yellow>
<yellow>value = sparse_dict.get(i, 0)  # returns 5</yellow>
â€¢ Pros: Fast lookups O(1), no memory wasted on zeros.
â€¢ Cons: Hash table overhead for keys and structure.

ð–¦¹ b) List of Tuples (Index-Value Pairs)
â€¢ Idea: Store as list of (index, value) pairs, sorted by index.
â€¢ Example:
<yellow>sparse_list = [(3, 5), (9, 10)]</yellow>
<pink># Access element at index i</pink>
<yellow>def get_value(i):</yellow>
<yellow>    for idx, val in sparse_list:</yellow>
<yellow>        if idx == i:</yellow>
<yellow>            return val</yellow>
<yellow>    return 0</yellow>
<yellow>print(get_value(3))  # Output: 5</yellow>
<yellow>print(get_value(5))  # Output: 0</yellow>
â€¢ Pros: Minimal memory overhead.
â€¢ Cons: Access O(n) if unsorted; can improve to O(log n) with sorted + binary search.

ð–¦¹ c) Compressed Storage Techniques
â€¢ CRS / CCS for matrices.
â€¢ Store: non-zero values, row/column indices, pointers to start of each row/column.
â€¢ Efficient for matrix operations in scientific computing or graphs.

5. Advantages
ð–¦¹ Saves memory drastically for large arrays with few non-zero elements.
ð–¦¹ Faster iteration over only meaningful elements.
ð–¦¹ Can handle datasets too large for memory in dense form.

6. Applications
ð–¦¹ Graph adjacency matrices: Most graphs are sparse.
â€¢ Example: Social networks with millions of users.
ð–¦¹ Scientific computations: Sparse matrices in physics, chemistry, engineering.
ð–¦¹ Machine learning: Sparse feature vectors in NLP, recommendation systems.

7. Memory Comparison
ð–¦¹ Dense Array: 1M elements â†’ 1M Ã— size_of(int) memory.
ð–¦¹ Sparse Dict: 10 non-zero â†’ 10 Ã— (index + value) + hash overhead.
ð–¦¹ Huge saving when non-zero elements << total size.

8. Key Takeaways
ð–¦¹ Sparse arrays are memory-efficient.
ð–¦¹ Choice of storage depends on lookup vs iteration needs:
â€¢ Dictionary â†’ fast random access.
â€¢ List of tuples â†’ less memory, slower access.
â€¢ CRS / CCS â†’ optimized for matrix operations.

SOURCE: Chapter 2 of The Book, "Machine Learning for Brain Disorders" by Johann Faouzi, Olivier Colliot
--BEGIN--CLASSICAL ML--22/11/2025
1. Nearest Neighbor: Intuition
 - Training Samples are converted into feature vectors
 - Clusters Observed
 - Class of a test sample is determined by checking the similarity extent with all the clusters
2. With Nearest Neighbor
 - Classification
 - Regression
3. Regression with Nearest Neighbor
 - Considering the nearest datapoints
 - And we take Weighted Average
 - Weighti = 1/Di^2 [Di is the distance]
		w·µ¢ = (1 / d(x_i, x)) / Œ£ (1 / d(x_j, x))
		≈∑ = (Œ£w·µ¢y·µ¢)/(Œ£w·µ¢)
4. Classification with Nearest Neighbor
 - Picking the most common label among the neighbors with more importance to closer ones
 - KNN with weighted voting is often better than simple majority voting
		≈∑ = argmax_C (Œ£w·µ¢1(y·µ¢ = C‚Çñ))
5. Finding Neighbors
 - Radius Neighbors: Points that are within the radius; possible to have 0 points
 - K-Nearest Neighbors: Fixed number of neighbors, no matter how far they are; possible to have irrelevant points
6. KNN Classification: Types
 - Unweighted (When distribution is uniform)
		Simple Majority Voting of Neighbors; We take the most common one
 - Weighted (When distribution is random)
		We weigh each points based on distance; We take the point with the highest value
7. Data Distribution
	Uniform: For any region, there will be roughly equal numbers of data-points
		‚¨§  ‚¨§  ‚¨§  ‚¨§
		‚¨§  ‚¨§  ‚¨§  ‚¨§
		‚¨§  ‚¨§  ‚¨§  ‚¨§
		‚¨§  ‚¨§  ‚¨§  ‚¨§
		‚¨§  ‚¨§  ‚¨§  ‚¨§
	Random: For any region, the count of data-points will be random (spaces, clusters)
		‚¨§   ‚¨§
		‚¨§   ‚¨§
		‚¨§   ‚¨§		
				‚¨§  ‚¨§
				‚¨§  ‚¨§
				‚¨§  ‚¨§
8. Halton and Hammersley Sequence: To generate evenly spread points for simulations
9. A Distance Metric should satisfy these conditions to be used as similarity metric
 - Non-negativity ‚Üí Distance is never negative.
		d(A, B) ‚â• 0
 - Identity ‚Üí Distance is 0 if and only if the points are identical.
		d(A, A) = 0, but d(A, B) > 0 if A ‚â† B
 - Symmetry ‚Üí Distance from A to B is the same as from B to A.
		d(A, B) = d(B, A)
 - Triangle Inequality ‚Üí The shortest path between two points is a straight line.
		d(A, B) + d(B, C) ‚â• d(A, C)	[a+b‚â•c; a+c‚â•b; b+c‚â•a]
10. Linear Regression
 - A linear model is a model that linearly combines the features
		Y^ = x^‚ä§w + B
11. To estimate w values,
 - Ordinary Least Squares Regression
	||y - Xw||¬≤‚ÇÇ
12. One wants to find the optimal parameters w‚ãÜ that minimize the cost function.
	w* = arg min J(w)
13. Ordinary Least Squares Loss Function is convex (local minimum = global minimum)
	Differentiable, implying that every local minimum has a null gradient.
14. So, Optimization for OLSR
	‚àáw‚ãÜ J = 0  
	‚àí2X·µÄy = 0  
	‚áí X·µÄXw‚ãÜ = X·µÄy  
	‚áí w‚ãÜ = (X·µÄX)‚Åª¬π X·µÄy
15. A Closed Formula (or closed-form solution) :Provides Exact Solution to a problem using a finite number of standard operations like:
 - Addition (+), subtraction (-)
 - Multiplication (√ó), division (√∑)
 - Exponentiation (^), logarithms (log), trigonometric functions (sin, cos, etc.)
 - No need iterative methods (like gradient descent)
16. Signed Distance
	y = sign(f(x)) = +1 if f(x) > 0  
	                 -1 if f(x) < 0
17. Logistic regression converts a raw score into a probability between 0 and 1 using the sigmoid function
	œÉ(u) = 1 / (1 + e^(-u))
18. Classification with Logistic Regression
 - Maximum Likelihood Estimation
19. Overfitting: When a model fits too well the training data and generalizes poorly to new data
	Underfitting: When a model does not capture well enough the characteristics of the training data, thus also generalizes poorly to new data
20. Bias: Error due to incorrect assumptions in the learning algorithm.
	Variance: Error due to excessive sensitivity to training data.
	Irreducible Error: Noise inherent in the data, which no model can eliminate
		Stock Market Prediction: Unexpected events introduce randomness
21. For a good model, variance and bias should be low
22. Bias Variance Trade-off: Bias is inversely proportional to Variance
23. Bias Variance Decomposition: 
	E[(y - ≈∑)¬≤] 
	= E[y¬≤ - 2y≈∑ + ≈∑¬≤] 
	= E[y¬≤] - 2yE[≈∑] + E[≈∑¬≤] 
	= E[y¬≤] - 2yE[≈∑] + E[≈∑¬≤] + E[≈∑]¬≤ - E[≈∑]¬≤ 
	= (E[≈∑] - y)¬≤ + E[≈∑¬≤] - E[≈∑]¬≤ 
	= (E[≈∑] - y)¬≤ + Var(≈∑) 
	= BIAS¬≤ + VARIANCE
24. Without ‚Ñì‚ÇÇ Regularization
Feature 1 (X1) | Feature 2 (X2) | Target (y)  
1.0           | 2.0           | 4.0  
2.0           | 4.1           | 8.1  
3.0           | 5.9           | 12.1  
4.0           | 8.2           | 16.2  
X2 ‚âà 2X1 (high collinearity).
Fitting: y = w1 * X1 + w2 * X2 + b  
- Because X1 and X2 are highly correlated, the model can assign arbitrarily large weights to either feature to fit the data
- Possible OLS solutions:  
  - w1 = 10, w2 = -6  
  - w1 = 4, w2 = 2  
  - w1 = -2, w2 = 8  
PBLM: Poor Generalization on New Data
25. With ‚Ñì‚ÇÇ Regularization (Ridge Regression)  
	L = Œ£ (y_i - (w1 * X1 + w2 * X2 + b))¬≤ + Œª (w1¬≤ + w2¬≤)  
where Œª controls the strength of the regularization.
Effect of Ridge Regression:  
  - It forces w1 and w2 to be small and stable.  
  - Instead of large fluctuations (e.g., w1 = 10, w2 = -6), Ridge might give:  
  - w1 = 3.8, w2 = 1.9  
  - This prevents over-reliance on a single feature and reduces collinearity issues.
26. ‚Ñì‚ÇÇ vs ‚Ñì‚ÇÅ
	The ‚Ñì‚ÇÇ penalty (Ridge Regression) prevents coefficients from becoming too large, but it does not force small values to become exactly zero.
	The ‚Ñì‚ÇÅ penalty (Lasso Regression) encourages some coefficients to shrink completely to zero, which helps in feature selection.
27. L1, L2, Elastic Net Penalties
L1 = Œ£ (y_i - ≈∑_i)¬≤ + Œª Œ£ |w_j|
L2 = Œ£ (y_i - ≈∑_i)¬≤ + Œª Œ£ w_j¬≤
ELASTIC = Œ£ (y_i - ≈∑_i)¬≤ + Œª‚ÇÅ Œ£ |w_j| + Œª‚ÇÇ Œ£ w_j¬≤ [Œª‚ÇÇ is 1-Œª‚ÇÅ]
27. SVM: Support Vector Machine finds the best line/hyper-plane that keeps the maximum possible distance from the closest points on both sides
28. SVC:
	- Closest Distance: Margin
	- Closest Points: Support vectors
29. If we use maximum margin possible, mid-point between support vectors, 
	üî¥üî¥üî¥üî¥üî¥  üü¢üü¢üü¢üü¢üü¢
	 It is called, MAXIMAL MARGIN CLASSIFIER
	 It is super sensitive to outliers
30. To get rid of outliers, we allow MISCLASSIFICATION
	If we allow Misclassification -> Soft Margin
	Best Soft Margin is found by CROSS VALIDATION
31. Non-linearity: Decision Function (Hyperplane) can be non-linear with the use of non-linear kernels: Kernel Trick
32. Soft Vector Machines uses Kernels: Polynomial, Linear, RBF
33. Polynomial Kernel
	K(a, b) = (axb + r)^d
	Say, r = 1/2 and d = 2
	K(a, b) = (axb + r)^d = (a, a^2, 1/2).(b, b^2, 1/2)
	Just for -> Nonlinear decision boundaries
from sklearn.svm import SVC
model = SVC(kernel='poly', degree=3, coef0=1, C=1.0)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
34. Radial Basis Function (RBF) kernel: Maps inputs into an infinite-dimensional space
35. Kernel Types
 - Linear Kernel:  
	  K(x, x') = x‚ä§x'  
	  Computes dot product; works for linearly separable data.  

 - Polynomial Kernel:  
	  K(x, x') = (Œ≥ x‚ä§x' + c‚ÇÄ)·µà  
	  Maps data to a higher-degree space; captures polynomial relationships.  

 - Sigmoid Kernel:  
	  K(x, x') = tanh(Œ≥ x‚ä§x' + c‚ÇÄ)  
	  Inspired by neural networks; applies a nonlinear transformation.  

 - Radial Basis Function (RBF) Kernel:  
	  K(x, x') = exp(-Œ≥ ‚à•x - x'‚à•¬≤)  
	  Uses Euclidean distance; maps data to infinite-dimensional space; captures complex patterns.  
36. Soft Vector Regression: Œµ-Tube: Œµ-Sensitive Loss: L_Œµ(y, ≈∑) = {
	0, if |y ‚àí ≈∑| ‚â§ Œµ
	|y ‚àí ≈∑| ‚àí Œµ, otherwise
}
In Œµ-insensitive loss, Œµ (epsilon) represents a tolerance margin within which prediction errors are ignored
37. MAE (Mean Absolute Error) = (1/n) * Œ£ |yi - ≈∑i|
  - Treats all errors equally (linear)
  - Less sensitive to outliers
  - May lead to multiple minima during optimization
  - Preferred when all errors should be treated equally
38. MSE (Mean Squared Error) = (1/n) * Œ£ (yi - ≈∑i)^2
  - Squares errors, making larger errors more significant
  - More sensitive to outliers
  - Provides smoother gradients for optimization
  - Preferred when large errors should be penalized more
39. Epsilon-Insensitive Loss: Ignores small errors (<œµ) and Linear penalty for large errors
40. Multinomial Logistic Regression
	- In multiclass classification, the goal is to classify an input into one of multiple classes (e.g., classifying an animal as a dog, cat, or rabbit)
	- Multiple Hyperplanes
	- Softmax on Signed Distances
	- Cross Entropy
41. Multiclass Classification - Multiple Binary Classification Strategies 
	- OvR
	- OvO
	- ECOC
42. One-vs-One (OvO) Strategy  
	Train three classifiers:
		Apple vs. Banana
		Apple vs. Orange
		Banana vs. Orange
	‚ôß Each classifier makes a prediction, and the class that wins the most votes is selected.
43. One-vs-Rest (OvR) Strategy
	Train three classifiers:
		Apple vs. (Banana + Orange)
		Banana vs. (Apple + Orange)
		Orange vs. (Apple + Banana)
	‚ôß Each classifier makes a prediction, and the class with the highest confidence score is the final prediction.
44. Error-Correcting Output Code (ECOC) Strategy
	- Instead of treating each class individually, classes are grouped into subsets for binary classification tasks
	- A codebook (matrix of -1 and +1 values) is used to assign class labels across multiple binary classifiers
45. Covariance: Cov(X1, X2): How X1 and X2 vary together
	Say, X1 -> Number of Words in Mail
		 X2 -> Number of Special Charaters
 - Positive Covariance
	X1 and X2 tend to increase or decrease together
	Example: As temperature increases, ice cream sales also increase.
 - Negative Covariance
	When X1 increases, X2 decreases, and vice versa.
	Example: As fuel efficiency (miles per gallon) increases, fuel consumption per mile decreases.
 - Zero Covariance
	No linear relationship between X1 and X2
	Example: The number of hours studied and the color of a student's shirt have no meaningful relationship
46. Variance: Var(X1): How X1 varies from Mean: How much X1 deviates from its mean
 - Low Variance (Small but Positive): The values of X1 are closely clustered around the mean
 - High Variance (Large Positive): The values of X1 are widely spread from the mean
47. Gaussian/Normal Distribution: üîî Bell-shaped Curve
	Example: Height of People against Age
	Values are concentrated around MEAN
48. When computing P(Banana | Lengthy) using Bayes' Theorem, we need to find P(Lengthy | Banana), which is the likelihood of a banana being lengthy. If banana lengths are normally distributed, we can model this with a Gaussian (normal) distribution.
	P(Length‚à£Banana) = N(Œº,œÉ^2) üçå
	P(Length | Banana) = (1 / sqrt(2 * pi * œÉ^2)) * exp(- ((Length - Œº)^2) / (2 * œÉ^2))
49. Decision Tree
 - Makes a statement
 - Checks if True or False
 - Branches accordingly
50. Decision Tree
	- The one that classifies: Classification Tree
	- The one that predicts a numeric value: Regression Tree
51. Pure and Impure
	A node is pure if it contains only a single class, and impure if it contains a mix of classes
52. Gini Impurity tells how pure/impure a node is.
	Gini After helps evaluate if a split improves classification.
	Gini Gain tells us which split is the best (higher Gini Gain = better split).
	Best Decision Trees maximize Gini Gain at every split
53. Dataset
	St	Hrs	Pass/Fail
	S1	8	Pass (P)
	S2	9	Pass (P)
	S3	7	Pass (P)
	S4	5	Pass (P)
	S5	3	Fail (F)
	S6	2	Fail (F)
	S7	10	Pass (P)
	S8	6	Pass (P)
	S9	4	Fail (F)
	S10	1	Fail (F)

 - Gini_before = 1‚àí(p^2 + F^2)
  = 1‚àí(0.6^2 +0.4^2)
  = 1‚àí(0.36+0.16)
  = 0.48
 - Group 1: "Study Hours < 5" (Lower Study Hours)
	 Gini_1 = 1‚àí(0^2 + 1^2)
	 = 1‚àí1
	 = 0
 - Group 2: "Study Hours >= 5" (Higher Study Hours)
	 Gini_2 = 1‚àí(1^2 + 0^2)
	 = 1‚àí1
	 = 0
 - Gini_after = ( (4/10) * Gini_1 ) + ( (6/10) * Gini_2 )
	Gini_after = 0 + 0 = 0
	Gini_gain = Gini_before - Gini_after
	Gini_gain = 0.48 - 0
	 = 0.48
	Perfect split! (Best possible case: Gini Impurity of 0 for both child nodes)
54. Random Forest
 - Each tree is trained on a random subset of training samples
 - Random sampling helps trees be less correlated with each other
 - At each split, only a random subset of features is considered
 - This prevents trees from relying on the same dominant features
 - Final prediction is obtained by combining all trees' outputs
For Classification:
	Hard Voting ‚Üí Most common class is chosen.
	Soft Voting ‚Üí Class with the highest average probability is chosen.
For Regression:
	The predicted value is the mean of all tree predictions.
	This reduces overfitting and improves accuracy!
55. Feature Correlation is a problem!
	> Feature1: Some trees might split first on "Number of Rooms"
	> Feature2: Others might split on "House Size."
	> Correlation: A bigger house usually has more rooms.
	> If features convey similar information, the trees end up making similar decisions
56. Extra Trees (Extremely Randomized Trees)
 - Add more randomness to reduce tree correlation
 - Instead of finding the best split for a feature, they pick a random threshold
 - The best split is then chosen from these random thresholds.
57. Centroid in Cluster: Mean of the data samples in the cluster
58. K Means Clustering
	> Randomly select centroids
	> Assign points nearby
	> Recompute Centroids (L2)
59. Dynamically Finding Number of Clusters
	We try to reduce Within Cluster Sum of Squares Criterion / Inertia
	> min {¬µ‚ÇÅ, ..., ¬µ‚Çñ} ‚àë(j=1 to k) ‚àë(x(i) ‚àà X‚±º) ||x(i) - ¬µ‚±º||¬≤
	There comes an Elbow Point, where the K value is changing the direction
	This is the point where K represents the actual number of clusters
60. We can also use Silhoutte Score instead of WCSSC
61. GMM: Gaussian Mixture Model
	Assumption: The data comes from multiple blobs; Each blob has a center (mean), spread (variance)
62. Clusters can Overlap: GMM tells, "This image belongs 70% to blob A, 30% to blob B"
63. Soft Assignments: Each image will have a probability of belonging to each animal group.
	One lion image might be:
	> 90% Lion Cluster
	> 8% Dog Cluster
	> 2% Cat Cluster
64. Gaussian/Blob Parameters
	Œº (mu), Mean ‚Äî center of the cluster
	Œ£ (sigma), Covariance ‚Äî spread and shape of cluster
	œÄ (pi), Mixing probability ‚Äî size of cluster
	It learns by finding the best values for these all above
65. Two Steps in GMM
	> Expectation Step: For each data point xi, We compute The Probability that xi came from Gaussian k
		r_ik = (pi_k * N(xi | mu_k, Sigma_k)) / (sum over j=1 to K of (pi_j * N(xi | mu_j, Sigma_j)))
	> Maximization Step: Update the parameters ùúáùëò, Œ£ùëò, ùúãùëò based on how much responsibility each point takes
66. GMM Steps
	1. Initialize K Gaussian components with random or KMeans-based means, covariances, and weights.
	2. For each data point, compute the responsibility (probability) for each Gaussian using current parameters.
	3. Update each Gaussian‚Äôs mean by taking the weighted average of data points based on responsibilities.
	4. Update each Gaussian‚Äôs covariance matrix using the weighted scatter of data points around the new mean.
	5. Update each Gaussian‚Äôs mixing weight by the average responsibility across all points.
	6. Check for convergence by evaluating the change in log-likelihood or stop after a maximum number of iterations.
	7. Once converged, output the final Gaussian parameters and the soft cluster assignments for each data point.
67. Eigen Vector: A special vector that, when acted upon by a matrix, only gets stretched or compressed (scaled by a scalar called an eigenvalue) but does not change its direction
	Av=Œªv
68. Curse of Dimensionality
	- More Features -> Sparsity of Datapoints
	- In HIGH Dimensions, the distance loses it's meaning
	- Higher Dimensional Datapoints formulate Hyper Sphere -> Angle Metric makes sense
	- The more sparse data makes models harder to train and more prone to overfitting capturing noise rather than the underlying patterns
69. More Spread - More Information
70. Principal Component Analysis: Aims to find a low-dimensional representation of a dataset while keeping a great share of the variance; It is unsupervised
71. Eigen Vectors tell us the principal components
	Eigen Values tell us how much variance (importance) each principal component explains
72. We take The Eigen Vector of Covariance Matrix
73. When we project a particular vector onto the eigen vector, the projected vector stretches (Spreads) | Any vector can have some spread (variance) when we project the data onto it. But, the spread is not maximum unless the vector is an eigenvector of the covariance matrix
--END-- data makes models harder to train and more prone to overfitting capturing noise rather than the underlying patterns
69. More Spread - More Information
70. Principal Component Analysis: Aims to find a low-dimensional representation of a dataset while keeping a great share of the variance; It is unsupervised
71. Eigen Vectors tell us the principal components
	Eigen Values tell us how much variance (importance) each principal component explains
72. We take The Eigen Vector of Covariance Matrix
73. When we project a particular vector onto the eigen vector, the projected vector stretches (Spreads) | Any vector can have some spread (variance) when we project the data onto it. But, the spread is not maximum unless the vector is an eigenvector of the covariance matrix
--END--
SOURCE: NOT TO BE DISCLOSED FOR NOW
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[06/04/2025]
1. Supervised (Labelled Dataset: Naive Bayes Classification)
   Unsupervised (No labels: K-Means Clustering)
2. Bias-Variance TradeOff with Model Complexity
		-> HIGH: Variance is HIGH
			During Training, Patterns captured well
		-> LOW : Variance is LOW
			Patterns may be missed
3. Regularizations: Limiting Model's Parameters freedom to adjust
	L1: shrink and possibly kill
		Loss_Lasso = sum((y_i - X_i * beta)^2) + lambda * sum(abs(beta_j))
	L2: shrink but don't kill
		Loss_Ridge = sum((y_i - X_i * beta)^2) + lambda * sum(beta_j^2)
	Elastic Net: Essence of both L1 and L2
		Loss_ElasticNet = sum((y_i - X_i * beta)^2) + lambda1 * sum(abs(beta_j)) + lambda2 * sum(beta_j^2)
4. Evaluation Metrics
	PRECISION: TP/(TP+FP)
	RECALL   : TP/(TP+FN)
	F1 SCORE : 2PR/(P+R)
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[07/04/2025]
5. Annotation: Labeling Data
 In computer vision, this often means drawing bounding boxes, assigning class labels, or segmenting objects in images
6. Importance of Annotation
	- A Good Training
	- Evaluation (Especially in Unsupervised Training Algorithms)
	- Accuracy (Deployment Readiness: Self Driving Cars)
7. Image segmentation: Divides an image into multiple parts or regions; To label each pixel
8. Types of Segmentation
   - Semantic: Doesn't differentiate between instances of the same object
   - Instance: Identifying individual instances of each object
   - Panoptic: Semantic(Stuff) + Instance(Things)
9. Annotation Tools
	- CVAT: Bounding boxes, polygons, keypoints, Smart Scissor, Automatic annotation using ML models
	- LabelImg
	- LabelBox
	- RoboFlow
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[08/04/2025]
10. Annotation Formats
	- COCO: Common Object in COntext
		{
	  "annotations": [{
	    "image_id": 1,
	    "bbox": [100, 200, 50, 80],
	    "category_id": 3
	  }]
 	 }
 	- YOLO: You Only Look Once
 		class_id x_center y_center width height: 0 0.5 0.5 0.2 0.3
 	- Pascal VOC
	    &lt;annotation&gt;  
	      &lt;object&gt;  
	        &lt;name&gt;cat&lt;/name&gt;  
	        &lt;bndbox&gt;  
	          &lt;xmin&gt;50&lt;/xmin&gt;&lt;ymin&gt;80&lt;/ymin&gt;&lt;xmax&gt;200&lt;/xmax&gt;&lt;ymax&gt;300&lt;/ymax&gt;  
	        &lt;/bndbox&gt;  
	      &lt;/object&gt;  
	    &lt;/annotation&gt;
11. Annotation Best Practices
	- Class balancing
	- Consistent labeling
	- Annotator QA

12. Compute Devices
	CPU: Central Processing Unit
	- - - - - - - - - - - - - - - - - - - - 
	GPU: Graphics Processing Unit
	 SIMT (In NVIDIA, 1 Warp=32 Threads)
 	 All threads in a warp execute the same instruction at the same time, that’s the SIMD part — but on different pieces of data
	- - - - - - - - - - - - - - - - - - - - 
	NPU: Neural Processing Unit
	 Laser-focused on deep learning by executing massive numbers of matrix multiplications and activation functions in parallel, using highly efficient hardware like tensor cores or systolic arrays. It’s optimized for low-precision arithmetic (like INT8/FP16), uses on-chip memory to minimize data movement, and processes data through predefined paths to achieve ultra-fast, energy-efficient inference, making it the brain behind AI in phones, cars, and edge devices
13. Parallelism = doing many things at the same time
	CPUs: 4–16 cores → can do a few things in parallel
	GPUs: Hundreds to thousands of tiny cores → made to do many things at once
	NPUs: Specialized cores → parallel for specific AI tasks like matrix math
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[09/04/2025]
14. Memory Bandwidth = how fast a processor can "move data" in and out of memory
	Narrow road = slow traffic = processor waits for data
	Wide road = lots of data flows = processor stays busy
	Measured in GB/s
15. Latency: Time taken to process a single input (e.g., 1 image)
16. Throughput: Number of inputs processed per second
17. Batch Size Effects
	Batch size = number of inputs given to the model at once
	Small batch
		✅ Low latency
		❌ Lower throughput
	Large batch
		✅ Higher throughput
		❌ Higher latency per input
	Choose based on use case:
		Real-time = small batch
		Server/batch jobs = large batch
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -[10/04/2025]
18. Inference Performance Metrics
----------------------------------------------------------------------------------------
| Property          | CPU                          | GPU                               |
|-------------------|------------------------------|-----------------------------------|
| Parallelism       | Low                          | High                              |
| Latency           | Low startup                  | High startup, low batch           |
| Throughput        | Low (DL tasks)               | High                              |
| Power Efficiency  | Efficient at low usage       | Higher power draw                 |
| Memory Bandwidth  | Low                          | High                              |
| Cost              | Cheaper                      | Costly, better perf/₹             |
|------------------|-------------------------------|-----------------------------------|
19. Metrics for Inference: FLOP, Latency, Throughput
20. FLOP: Floating Point Operations per Second
	If a GPU can do 1 trillion calculations per second, we say it has 1 TFLOP (Tera FLOP = trillion FLOPs)
	NOTE: Not always reflective of actual performance unless the compute is fully utilized
	Example: You run a deep learning model to classify one image (batch size = 1) on a GPU. That model is small, so it only uses a few of the GPU’s cores. Even though the GPU has 100 TFLOPs, you might only be using 5 TFLOPs because the rest of the GPU is sitting idle.
21. Simultaneous Multithreading/Hyperthreading: A single physical CPU core acts like two logical cores to handle multiple threads simultaneously and improve parallel task execution efficiency without increasing the number of physical cores
